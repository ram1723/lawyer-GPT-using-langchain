{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prasa\\Downloads\\fakelogodetect\\venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent directory does not exist. Initializing vector store...\n",
      "\n",
      "--- Document Chunks Information ---\n",
      "Number of document chunks: 957501\n",
      "\n",
      "--- Creating embeddings ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prasa\\AppData\\Local\\Temp\\ipykernel_23648\\1899859021.py:42: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  huggingface_embeddings = HuggingFaceEmbeddings(\n",
      "c:\\Users\\prasa\\Downloads\\fakelogodetect\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\prasa\\Downloads\\fakelogodetect\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prasa\\Downloads\\fakelogodetect\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prasa\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\prasa\\Downloads\\fakelogodetect\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Finished creating embeddings ---\n",
      "\n",
      "--- Creating and persisting vector store ---\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Finished creating embeddings ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Creating and persisting vector store ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m     db \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhuggingface_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersistent_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Finished creating and persisting vector store ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prasa\\Downloads\\fakelogodetect\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:878\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    876\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    877\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prasa\\Downloads\\fakelogodetect\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:842\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    836\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[0;32m    837\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[0;32m    838\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    839\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    840\u001b[0m         )\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 842\u001b[0m     \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32mc:\\Users\\prasa\\Downloads\\fakelogodetect\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32mc:\\Users\\prasa\\Downloads\\fakelogodetect\\venv\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:119\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m    116\u001b[0m         texts, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_kwargs\n\u001b[0;32m    117\u001b[0m     )\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "class TextLoader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.filepath, 'r', encoding='utf-8') as file:\n",
    "            # Create Document objects instead of dictionaries\n",
    "            return [Document(line.strip(), {}) for line in file if line.strip()]\n",
    "\n",
    "load_dotenv()\n",
    "current_dir = os.getcwd()\n",
    "cases_dir = os.path.join(current_dir, \"cases\")\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")\n",
    "\n",
    "if not os.path.exists(persistent_directory):\n",
    "    print(\"Persistent directory does not exist. Initializing vector store...\")\n",
    "    if not os.path.exists(cases_dir):\n",
    "        raise FileNotFoundError(f\"The directory {cases_dir} does not exist. Please check the path.\")\n",
    "    \n",
    "    case_files = [f for f in os.listdir(cases_dir) if f.endswith(\".txt\")]\n",
    "    documents = []\n",
    "    for file in case_files:\n",
    "        file_path = os.path.join(cases_dir, file)\n",
    "        loader = TextLoader(file_path)\n",
    "        book_docs = loader.load()\n",
    "        for doc in book_docs:\n",
    "            doc.metadata['source'] = file\n",
    "            documents.append(doc)\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    print(\"\\n--- Document Chunks Information ---\")\n",
    "    print(f\"Number of document chunks: {len(docs)}\")\n",
    "    print(\"\\n--- Creating embeddings ---\")\n",
    "    huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "    print(\"\\n--- Finished creating embeddings ---\")\n",
    "    print(\"\\n--- Creating and persisting vector store ---\")\n",
    "    db = Chroma.from_documents(\n",
    "            docs, huggingface_embeddings, persist_directory=persistent_directory)\n",
    "    print(\"\\n--- Finished creating and persisting vector store ---\")\n",
    "else:\n",
    "    print(\"Vector store already exists. No need to initialize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "current_dir = os.getcwd()\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata\")\n",
    "embeddings=HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)\n",
    "legal_retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 3, \"score_threshold\":0.4},\n",
    ")\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "qa_system_prompt=(\n",
    "    \"You are an expert assissant for a lawyer,Use the following pieces of retrived content to answer the questions.Give a detailed walkin about the query asked and clearly show how the query and information in the query has been used in the retrived documents as well. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, just \"\n",
    "    \"reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, legal_retriever, contextualize_q_prompt\n",
    ")\n",
    "qa_chain=create_stuff_documents_chain(llm,qa_prompt)\n",
    "rag_chain=create_retrieval_chain(history_aware_retriever, qa_chain)\n",
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        # Display the AI's response\n",
    "        print(f\"AI: {result['answer']}\")\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(SystemMessage(content=result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from googleapiclient.discovery import build\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY=os.getenv('GOOGLE_API_KEY')\n",
    "CX = os.getenv('CX')\n",
    "def fetch_news(query,count=10):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_API_KEY)\n",
    "    res = service.cse().list(q=query, cx=CX, num=count).execute()\n",
    "    return res.get('items', [])\n",
    "def get_news_articles(query):\n",
    "    print(f\"Searching for articles related to: {query}\")\n",
    "    search_results = fetch_news(query)\n",
    "    articles = []\n",
    "    for result in search_results:\n",
    "        title = result.get('title')\n",
    "        link = result.get('link')\n",
    "        snippet = result.get('snippet')\n",
    "        articles.append({\"title\": title, \"link\": link, \"snippet\": snippet})\n",
    "    return articles\n",
    "#write the creation and pf the news db\n",
    "def embed_and_store_news_articles(articles, persistent_directory_news):\n",
    "    news_documents = []\n",
    "    for article in articles:\n",
    "        news_text = article['title'] + \" \" + article['snippet']\n",
    "        news_documents.append(news_text)\n",
    "        news_db = Chroma.from_documents(\n",
    "        news_documents, embedding_function=embeddings, persist_directory=persistent_directory_news)\n",
    "        return news_db\n",
    "def integrate_news_into_system(query):\n",
    "    articles = get_news_articles(query)\n",
    "    current_dir = os.getcwd()\n",
    "    news_persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata_news\")\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        print(\"Persistent directory does not exist. Initializing vector store...\")\n",
    "        if not os.path.exists(cases_dir):\n",
    "            raise FileNotFoundError(f\"The directory {cases_dir} does not exist. Please check the path.\")\n",
    "        news_db = embed_and_store_news_articles(articles, news_persistent_directory)\n",
    "    return news_db\n",
    "news_retriever = news_db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 3, \"score_threshold\": 0.4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, legal_retriever, news_retriever):\n",
    "    legal_results = legal_retriever.retrieve(query)\n",
    "    news_results = news_retriever.retrieve(query)\n",
    "    combined_results = legal_results + news_results\n",
    "    return combined_results\n",
    "combined_results = hybrid_search(query, legal_retriever, news_retriever)\n",
    "answer = rag_chain.invoke({\"input\": query, \"chat_history\": [], \"documents\": combined_results})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined code\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "CX = os.getenv('CX')\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "current_dir = os.getcwd()\n",
    "cases_dir = os.path.join(current_dir, \"cases\")\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")\n",
    "news_persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata_news\")\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "class TextLoader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.filepath, 'r', encoding='utf-8') as file:\n",
    "            return [Document(line.strip(), {}) for line in file if line.strip()]\n",
    "if not os.path.exists(persistent_directory):\n",
    "    print(\"Persistent directory does not exist. Initializing vector store...\")\n",
    "    case_files = [f for f in os.listdir(cases_dir) if f.endswith(\".txt\")]\n",
    "    documents = []\n",
    "    for file in case_files:\n",
    "        file_path = os.path.join(cases_dir, file)\n",
    "        loader = TextLoader(file_path)\n",
    "        book_docs = loader.load()\n",
    "        for doc in book_docs:\n",
    "            doc.metadata['source'] = file\n",
    "            documents.append(doc)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    db = Chroma.from_documents(\n",
    "        docs, embeddings, persist_directory=persistent_directory)\n",
    "else:\n",
    "    print(\"Vector store already exists. No need to initialize.\")\n",
    "\n",
    "def fetch_news(query, count=10):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_API_KEY)\n",
    "    res = service.cse().list(q=query, cx=CX, num=count).execute()\n",
    "    return res.get('items', [])\n",
    "\n",
    "def get_news_articles(query):\n",
    "    search_results = fetch_news(query)\n",
    "    articles = []\n",
    "    for result in search_results:\n",
    "        title = result.get('title')\n",
    "        link = result.get('link')\n",
    "        snippet = result.get('snippet')\n",
    "        articles.append({\"title\": title, \"link\": link, \"snippet\": snippet})\n",
    "    return articles\n",
    "\n",
    "def embed_and_store_news_articles(articles, persistent_directory_news):\n",
    "    news_documents = []\n",
    "    for article in articles:\n",
    "        news_text = article['title'] + \" \" + article['snippet']\n",
    "        news_documents.append(Document(news_text, {}))\n",
    "    news_db = Chroma.from_documents(\n",
    "        news_documents, embeddings, persist_directory=persistent_directory_news)\n",
    "    return news_db\n",
    "def integrate_news_into_system(query):\n",
    "    articles = get_news_articles(query)\n",
    "    if not os.path.exists(news_persistent_directory):\n",
    "        print(\"Persistent directory for news does not exist. Initializing vector store for news...\")\n",
    "        news_db = embed_and_store_news_articles(articles, news_persistent_directory)\n",
    "    else:\n",
    "        print(\"News vector store already exists. No need to initialize.\")\n",
    "        news_db = Chroma(persist_directory=news_persistent_directory, embedding_function=embeddings)\n",
    "    \n",
    "    return news_db\n",
    "\n",
    "# Hybrid search across legal and news documents\n",
    "def hybrid_search(query, legal_retriever, news_retriever):\n",
    "    legal_results = legal_retriever.retrieve(query)\n",
    "    news_results = news_retriever.retrieve(query)\n",
    "    if pdf.retriver:\n",
    "        combined_results = legal_results + pdf_results\n",
    "    else:\n",
    "        combined_results = legal_results + news_results\n",
    "    return combined_results\n",
    "\n",
    "legal_retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 5, \"score_threshold\": 0.4})\n",
    "def integrate_news_and_search(query):\n",
    "    news_db = integrate_news_into_system(query)  # Pass the query dynamically\n",
    "    news_retriever = news_db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 5, \"score_threshold\": 0.4})\n",
    "    combined_results = hybrid_search(query, legal_retriever, news_retriever)\n",
    "    return combined_results\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "qa_system_prompt = (\n",
    "    \"You are an expert assistant for a lawyer. Use the following pieces of retrieved content \"\n",
    "    \"to answer the questions. Give a detailed explanation and clearly show how the query and \"\n",
    "    \"information in the query have been used in the retrieved documents as well.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, just \"\n",
    "    \"reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, legal_retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
    "\n",
    "# Chat function\n",
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        combined_results = integrate_news_and_search(query)\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history, \"documents\": combined_results})\n",
    "        # Display the AI's response\n",
    "        print(f\"AI: {result['answer']}\")\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(SystemMessage(content=result[\"answer\"]))\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if to upload file \n",
    "#write for th eupload of the file \n",
    "import fitz\n",
    "def text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text(\"text\")\n",
    "    return text\n",
    "def embed_pdf_text_to_store(text, embedding_model, vector_store_path):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    pdf_docs = text_splitter.split_documents([Document(page_content=text, metadata={})])\n",
    "    db = Chroma.from_documents(\n",
    "        pdf_docs, embeddings, persist_directory=persistent_directory)\n",
    "    return db\n",
    "pdf_retriver=db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 5, \"score_threshold\": 0.4})\n",
    "def handle_pdf_qa(query, pdf_retriever):\n",
    "    pdf_results = pdf_retriever.retrieve(query)\n",
    "    return pdf_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_case_outcome(document, query_outcome):\n",
    "    case_outcome = llm.predict(f\"Does this case support or contradict the following outcome: {query_outcome}? \\n\\nCase: {document.page_content}\")\n",
    "    return case_outcome\n",
    "\n",
    "def classify(docs,query):\n",
    "    classifications = []\n",
    "    for doc in docs:\n",
    "        outcome = classify_case_outcome(doc, query['legal_position'])\n",
    "        classifications.append({\n",
    "            'document': doc,\n",
    "            'classification': outcome\n",
    "        })\n",
    "    return classifications\n",
    "def identify_contradictions_and_supports(classified_docs, query_outcome):\n",
    "    contradictory_cases = []\n",
    "    supported_cases=[]\n",
    "    for item in classified_docs:\n",
    "        if item['classification'] == 'Contradictory':\n",
    "            contradictory_cases.append(item['document'])\n",
    "        elif item['classification']=='Supportive':\n",
    "            supported_cases.append(item['document'])\n",
    "    return contradictory_cases,supported_cases\n",
    "def supportive_and_contradiction_explanation(contradictions,supports, query):\n",
    "    supported_explanations = []\n",
    "    condractive_explanations=[]\n",
    "    for case in supports:\n",
    "        support_explanation = llm.predict(f\"Explain why this case supports the following legal position: {query['legal_position']}. \\n\\nCase: {case.page_content}\")\n",
    "        supported_explanations.append(support_explanation)\n",
    "    for case in contradictions:\n",
    "        contradict_explanation = llm.predict(f\"Explain why this case contradicts the following legal position: {query['legal_position']}. \\n\\nCase: {case.page_content}\")\n",
    "        condractive_explanations.append(contradict_explanation)\n",
    "    return supported_explanations,condractive_explanations\n",
    "\n",
    "def retrive_and_classify_docs_fetched(query):\n",
    "    # Fetch documents from the database\n",
    "    docs=legal_retriever\n",
    "    # Classify the documents\n",
    "    classified_documents=classify(docs)\n",
    "    contradictions,supports=identify_contradictions_and_supports(classified_documents,query['legal_position'])\n",
    "    supportive_explanations,contradiction_explanations = supportive_and_contradiction_explanation(contradictions,supports, query)\n",
    "    return{\n",
    "       'Supportive':supports,\n",
    "       'Supportive_explanations':supportive_explanations,\n",
    "       'Contradictory': contradictions,\n",
    "       'contradiction_explanations':contradiction_explanations\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import VectorStoreRetriever\n",
    "from langchain.chains import LLMChain\n",
    "import openai\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "current_dir = os.getcwd()\n",
    "cases_dir = os.path.join(current_dir, \"cases\")\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")\n",
    "vector_store = Chroma(persist_directory=persistent_directory, embedding_function=embedding_model)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "GOOGLE_CSE_ID = os.getenv('CX')\n",
    "def process_pdf(pdf_path):\n",
    "    pdf_loader = PDFLoader(file_path=pdf_path)\n",
    "    pdf_texts = pdf_loader.load_and_split(text_splitter=text_splitter)\n",
    "    vector_store.add_documents(pdf_texts)\n",
    "    return pdf_texts\n",
    "def embed_pdf_text_to_store(pdf_path):\n",
    "    pdf_texts = process_pdf(pdf_path)\n",
    "    vector_store.add_documents(pdf_texts)\n",
    "    print(f\"PDF text embedded and added to vector store.\")\n",
    "\n",
    "def retrieve_legal_cases(query):\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 5, \"score_threshold\": 0.4})\n",
    "    relevant_cases = retriever.get_relevant_documents(query)\n",
    "    return relevant_cases\n",
    "\n",
    "def fetch_news(query):\n",
    "    import requests\n",
    "    search_url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={GOOGLE_API_KEY}&cx={GOOGLE_CSE_ID}\"\n",
    "    response = requests.get(search_url)\n",
    "    articles = response.json().get('items', [])\n",
    "    news_texts = [article[\"snippet\"] for article in articles]\n",
    "\n",
    "    news_documents = [{\"content\": text, \"metadata\": {\"source\": \"news\"}} for text in news_texts]\n",
    "    vector_store.add_documents(news_documents)\n",
    "    return news_texts, articles\n",
    "\n",
    "def integrate_news_and_search(query):\n",
    "    news_texts, articles = fetch_news(query)\n",
    "    return news_texts, articles\n",
    "\n",
    "def classify_case_outcome(case_text, legal_position):\n",
    "    \"\"\"Classify if a case is supportive or contradictory to a given legal position.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    The legal position is: \"{legal_position}\"\n",
    "    Based on the given legal position, classify the following case as either 'supportive' or 'contradictory':\n",
    "\n",
    "    {case_text}\n",
    "\n",
    "    Respond with just 'supportive' or 'contradictory'.\n",
    "    \"\"\"\n",
    "    classification = llm.predict(prompt)\n",
    "    return classification.strip()\n",
    "\n",
    "def classify(retrieved_cases, legal_position):\n",
    "    \"\"\"Classify cases into supportive and contradictory categories.\"\"\"\n",
    "    supportive_cases = []\n",
    "    contradictory_cases = []\n",
    "\n",
    "    for case in retrieved_cases:\n",
    "        case_text = case['content']\n",
    "        outcome = classify_case_outcome(case_text, legal_position)\n",
    "\n",
    "        if outcome == 'supportive':\n",
    "            supportive_cases.append(case)\n",
    "        else:\n",
    "            contradictory_cases.append(case)\n",
    "\n",
    "    return supportive_cases, contradictory_cases\n",
    "\n",
    "def generate_case_explanation(cases, classification):\n",
    "    \"\"\"Generate explanation for supportive/contradictory cases.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    The following cases are classified as {classification}:\n",
    "    \n",
    "    {cases}\n",
    "\n",
    "    Provide an explanation of why these cases are {classification}.\n",
    "    \"\"\"\n",
    "    explanation = llm.predict(prompt)\n",
    "    return explanation\n",
    "\n",
    "\n",
    "def create_final_response(query, supportive_cases, contradictory_cases, news_articles):\n",
    "    \"\"\"Create the final response combining supportive/contradictory cases and news.\"\"\"\n",
    "    supportive_explanation = generate_case_explanation(supportive_cases, \"supportive\")\n",
    "    contradictory_explanation = generate_case_explanation(contradictory_cases, \"contradictory\")\n",
    "\n",
    "    response = f\"\"\"\n",
    "    Query: {query}\n",
    "\n",
    "    Supportive Cases:\n",
    "    {supportive_explanation}\n",
    "\n",
    "    Contradictory Cases:\n",
    "    {contradictory_explanation}\n",
    "\n",
    "    Related News:\n",
    "    {news_articles}\n",
    "    \"\"\"\n",
    "\n",
    "    return response\n",
    "\n",
    "def handle_query(query, pdf_path=None):\n",
    "    # Step 1: Handle PDF Upload if applicable\n",
    "    if pdf_path:\n",
    "        embed_pdf_text_to_store(pdf_path)\n",
    "\n",
    "    # Step 2: Retrieve legal cases from the vector store\n",
    "    retrieved_cases = retrieve_legal_cases(query)\n",
    "\n",
    "    # Step 3: Fetch related news articles\n",
    "    news_texts, articles = integrate_news_and_search(query)\n",
    "\n",
    "    # Step 4: Classify cases into supportive or contradictory categories\n",
    "    legal_position = query  # In a more complex system, you'd extract the position differently\n",
    "    supportive_cases, contradictory_cases = classify(retrieved_cases, legal_position)\n",
    "\n",
    "    # Step 5: Generate the final response\n",
    "    response = create_final_response(query, supportive_cases, contradictory_cases, articles)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from langchain import rag_chain  # Assuming you have imported and initialized this.\n",
    "import PyPDF2\n",
    "app = Flask(__name__)\n",
    "chat_history = []\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    data = request.get_json()\n",
    "    query = data.get('query')\n",
    "\n",
    "    # Process query using your RAG chain\n",
    "    result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "    chat_history.append({\"type\": \"user\", \"content\": query})\n",
    "    chat_history.append({\"type\": \"gpt\", \"content\": result['answer']})\n",
    "\n",
    "    return jsonify({\"response\": result['answer']})\n",
    "@app.route('/upload-pdf', methods=['POST'])\n",
    "def upload_pdf():\n",
    "    if 'pdf' not in request.files:\n",
    "        return jsonify({'message': 'No PDF file uploaded'}), 400\n",
    "\n",
    "    pdf_file = request.files['pdf']\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    \n",
    "    # Example: Extracting text from the first page\n",
    "    page = pdf_reader.pages[0]\n",
    "    text = page.extract_text()\n",
    "\n",
    "    # Process the text as needed and return a response\n",
    "    return jsonify({'message': 'PDF uploaded successfully!', 'text': text})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
